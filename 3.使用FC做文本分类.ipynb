{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Pytorch做文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类是自然语言处理中最基本的任务之一，我们可以根据已有的文本分类结果进行有监督训练后进行分类，也可以根据一定的要求将文本进行聚类。我们今天做一个最简单的基于全连接层的词语情感分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们首先来读取word2vec的矩阵，这个词向量矩阵是从Chinese-Word-Vectors提供的词向量矩阵中提取的，大小比较小，读取很快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_length:  9109\n",
      "emb_size:  300\n",
      "， (300,)\n"
     ]
    }
   ],
   "source": [
    "char_list = []\n",
    "emb_list = []\n",
    "\n",
    "# 读取切分好的一行，返回词和词向量（numpy的矩阵）\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "with open('素材\\sgns.wiki.char', 'r', encoding='utf-8') as emb_file:\n",
    "    # 文件的开头是词表长度和词嵌入维度\n",
    "    dict_length, emb_size = emb_file.readline().rstrip().split()\n",
    "    print('dict_length: ', dict_length)\n",
    "    print('emb_size: ', emb_size)\n",
    "    dict_length, emb_size = int(dict_length), int(emb_size)\n",
    "    # 对每一行做处理，结果存到顺序词典中\n",
    "    emb = collections.OrderedDict(get_coefs(*l.rstrip().split()) for l in emb_file.readlines())\n",
    "for k, v in emb.items():\n",
    "    print(k, v.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec会将相似的词放在一起，我们来测试一下。我用的这个word2vec预训练词向量是基于词训练的，但我删掉了所有词只剩单字，所以单字上的找相似字的效果表现不是很好。一般用余弦相似度来确定两个向量之间的相似度。\n",
    "$$\\cos(\\theta)=\\frac{x\\cdot y}{\\parallel x\\parallel\\times\\parallel y\\parallel}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "羊\n",
      "鲈\n",
      "疲\n"
     ]
    }
   ],
   "source": [
    "def cal_cos_similarity(vec1, vec2):\n",
    "    vec1 = np.mat(vec1)\n",
    "    vec2 = np.mat(vec2)\n",
    "    num = float(vec1 * vec2.T)\n",
    "    denom = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    # 防止除零错误\n",
    "    eps = 1e-5\n",
    "    sim = num / (denom + eps)\n",
    "    return sim\n",
    "\n",
    "def cal_char_similar(c):\n",
    "    c_vec = emb.get(c)\n",
    "    most_similar = ''\n",
    "    max_cos = -1\n",
    "    for k, v in emb.items():\n",
    "        if k == c:\n",
    "            continue\n",
    "        tmp = cal_cos_similarity(v, c_vec)\n",
    "        if tmp > max_cos:\n",
    "            max_cos = tmp\n",
    "            most_similar = k\n",
    "    return most_similar\n",
    "\n",
    "print(cal_char_similar('牛'))\n",
    "print(cal_char_similar('鱼'))\n",
    "print(cal_char_similar('难'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道，电脑无法直接处理文字，或者说，比较难以通过“文字”来找到对应的输入，因此我们在处理文本的时候，往往需要一个工具，将文本中的词或者字映射到一个数字ID，我们一般叫这个工具Tokenizer，这个没有太好的中文翻译，一般翻译为分词器，但是中文和英语不同，中文往往是多个字组成一个词，所以分词器应该是Segmenter。  \n",
    "举个例子：\n",
    "- Tokenizer： 翻/译/中/遇/到/的/难/点\n",
    "- Segmenter:  翻译/中/遇到/的/难点\n",
    "\n",
    "首先我们来生成一个Tokenizer类，很多时候Tokenizer也负责切分文本的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    # 初始化的时候读取词表\n",
    "    def __init__(self, vocab_list):\n",
    "        self.vocab = self.load_vocab(vocab_list)\n",
    "        for i, (k, v) in enumerate(self.vocab.items()):\n",
    "            if i > 9:\n",
    "                break\n",
    "            print(k, v)\n",
    "    \n",
    "    # 读取词表\n",
    "    def load_vocab(self, vocab_list):\n",
    "        # 我们一般使用顺序字典来存储词表，这样能够保证历遍时index升序排列\n",
    "        vocab = collections.OrderedDict()\n",
    "        # 一般我们使用'UNK'来表示词表中不存在的词，放在0号index上\n",
    "        vocab['UNK'] = 0\n",
    "        index = 1\n",
    "        # 依次插入词\n",
    "        for token in vocab_list:\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "        return vocab\n",
    "\n",
    "    # 将单个字/词转换为数字id\n",
    "    def token_to_id(self, token):\n",
    "        # 不在词表里的词\n",
    "        if token not in self.vocab.keys():\n",
    "            return self.vocab['UNK']\n",
    "        else:\n",
    "            return self.vocab[token]\n",
    "\n",
    "    # 将多个字/词转换为数字id\n",
    "    def tokens_to_ids(self, tokens):\n",
    "        ids_list = list(map(self.token_to_id, tokens))\n",
    "        return ids_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的词表就是之前的词向量里的“词”，所以我们可以现在来生成Tokenizer实例了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK 0\n",
      "， 1\n",
      "的 2\n",
      "。 3\n",
      "、 4\n",
      "和 5\n",
      "在 6\n",
      "年 7\n",
      "“ 8\n",
      "了 9\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(emb.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成tokenizer实例的同时，我们也能够生成我们的词嵌入矩阵了。词嵌入矩阵就是第i行对应的就是编号为i的词的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.150785  0.120591 -0.220204 ... -0.532687 -0.066914 -0.100728]\n",
      " [-0.251355  0.234742 -0.169728 ... -0.527221 -0.229531  0.180781]\n",
      " ...\n",
      " [ 0.014415  0.001792 -0.064646 ... -0.113399 -0.045758 -0.005999]\n",
      " [ 0.012528 -0.007813 -0.001642 ... -0.08116  -0.054745  0.012526]\n",
      " [ 0.031915  0.002482 -0.008512 ... -0.102545 -0.055438  0.023618]]\n",
      "(9110, 300)\n"
     ]
    }
   ],
   "source": [
    "# 生成一个全0矩阵，大小为（词典长度+1，嵌入维度）\n",
    "emb_matrix = np.zeros((1 + dict_length, emb_size), dtype='float32')\n",
    "\n",
    "for word, id in tokenizer.vocab.items():\n",
    "    emb_vector = emb.get(word)\n",
    "    if emb_vector is not None:\n",
    "        # 将编号为id的词的词向量放在id行上\n",
    "        emb_matrix[id] = emb_vector\n",
    "print(emb_matrix)\n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来还是一样，我们来搭建NN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifierNet(\n",
      "  (emb): Embedding(9110, 300)\n",
      "  (relu): ReLU()\n",
      "  (linear1): Linear(in_features=1500, out_features=100, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (linear2): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (linear3): Linear(in_features=20, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n",
      "152162 parameters is trainable.\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearClassifierNet(nn.Module):\n",
    "    def __init__(self, seq_length, label_len):\n",
    "        super(LinearClassifierNet, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.label_len = label_len\n",
    "        # 第一层是一个嵌入层，输入为(batch_size, seq_length),输出为(batch_size, seq_length, emb_size)\n",
    "        # 嵌入层如果使用了from_pretrained，会关掉自动梯度，也就是变得不能训练。如果需要可以手动开启。\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.tensor(emb_matrix))\n",
    "        self.emb_size = self.emb.embedding_dim\n",
    "        # ReLU层无参数，可以共用\n",
    "        self.relu = nn.ReLU()\n",
    "        # 第一个全连接层，输入为(batch_size, seq_length*emb_size)，输出为(batch_size, 100)\n",
    "        self.linear1 = nn.Linear(seq_length * emb_size, 100)\n",
    "        # dropout层\n",
    "        # Batch Norm和dropout一般不同时使用，如果你想同时用的话，顺序一般是：\n",
    "        # CONV/FC -> BatchNorm -> ReLU/Sigmoid/tanh/GeLU(或者别的激活函数) -> Dropout -> ...\n",
    "        # 参考论文 https://arxiv.org/abs/1502.03167\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # 第二个全连接层，输入为(batch_size, 100)，输出为(batch_size, 20)\n",
    "        self.linear2 = nn.Linear(100, 20)\n",
    "        # 第三个全连接层，输入为(batch_size, 20)，输出为(batch_size, label_len)\n",
    "        self.linear3 = nn.Linear(20, self.label_len)\n",
    "        # softmax分类层\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        # 使用交叉熵损失函数\n",
    "        # 交叉熵损失函数实际上等于nn.Softmax+nn.NLLLoss（负对数似然损失），所以用这个损失的时候不需要先过softmax层\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # forward 定义前向传播，参数不同，输出结果也不同\n",
    "    # x = 整体输入是(batch_size, seq_length), y = gold_labels\n",
    "    def forward(self, x, y=None):\n",
    "        # 嵌入层，输出为(batch_size, seq_length, emb_size)\n",
    "        x = self.emb(x)\n",
    "        # 把输出的后两维拍平，变成一维\n",
    "        x = x.view(-1, self.seq_length * self.emb_size)\n",
    "        # 过第一个线性层\n",
    "        # 输入为(batch_size, seq_length*emb_size)，输出为(batch_size, 100)\n",
    "        x = self.linear1(x)\n",
    "        # 非线性激活函数\n",
    "        x = self.relu(x)\n",
    "        # dropout层\n",
    "        # dropout是在参数很多的层用，参数越多，p可以越大\n",
    "        x = self.dropout(x)\n",
    "        # 过第二个线性层\n",
    "        x = self.linear2(x)\n",
    "        # 非线性激活函数\n",
    "        x = self.relu(x)\n",
    "        # 过第三个线性层\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        # 如果没有输入y，那么是在预测，我们返回分类的结果\n",
    "        if y is None:\n",
    "            return self.softmax(x)\n",
    "        # 如果有输入y，那么是在训练，我们返回损失函数的值\n",
    "        else:\n",
    "            return self.loss(x, y)\n",
    "        \n",
    "# 我们做的是词语的情感分析，最长为5\n",
    "seq_length = 5\n",
    "# 情感只有正负两类\n",
    "label_len = 2\n",
    "\n",
    "model = LinearClassifierNet(seq_length, label_len)\n",
    "# 使用print可以打印出网络的结构\n",
    "print(model)\n",
    "\n",
    "# 有多少个可以训练的参数\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(str(total_trainable_params), 'parameters is trainable.')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们来做数据的操作。一般来说我建议大家建立两个类，一个类放原始的数据和标签，一个类放处理完毕的数据和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 原始数据和标签 \n",
    "class data_example:\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "# 处理完毕的数据和标签\n",
    "class data_feature:\n",
    "    def __init__(self, ids, label_id):\n",
    "        self.ids = ids\n",
    "        self.label = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of example: 2085\n",
      "爱 positive\n",
      "爱不忍释 positive\n",
      "爱不释手 positive\n"
     ]
    }
   ],
   "source": [
    "# 读原始数据\n",
    "examples = []\n",
    "with open('素材/sentiment/正面情感词语（中文）.txt', 'r', encoding='gbk') as pos_file:\n",
    "    for line in pos_file:\n",
    "        line = line.strip()\n",
    "        examples.append(data_example(line, 'positive'))\n",
    "with open('素材/sentiment/负面情感词语（中文）.txt', 'r', encoding='gbk') as pos_file:\n",
    "    for line in pos_file:\n",
    "        line = line.strip()\n",
    "        examples.append(data_example(line, 'negative'))\n",
    "\n",
    "print('num of example: %d' % len(examples))\n",
    "for i in range(3):\n",
    "    print(examples[i].text, examples[i].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们将原始数据转换成处理完毕的数据和标签。这么做主要是为了解耦。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[332, 0, 0, 0, 0] 1\n",
      "[332, 57, 2028, 2070, 0] 1\n",
      "[332, 57, 2070, 444, 0] 1\n"
     ]
    }
   ],
   "source": [
    "# 处理原始数据\n",
    "def convert_example_to_feature(examples):\n",
    "    features = []\n",
    "    for i in examples:\n",
    "        # 使用tokenizer将字符串转换为数字id\n",
    "        ids = tokenizer.tokens_to_ids(i.text)\n",
    "        # 我们规定了最大长度，超过了就切断，不足就补齐（一般补unk，也就是这里的[0]，也有特殊补位符[PAD]之类的）\n",
    "        if len(ids) > seq_length:\n",
    "            ids = ids[: seq_length]\n",
    "        else:\n",
    "            ids = ids + [0] * (seq_length - len(ids))\n",
    "        # 如果这个字符串全都不能识别，那就放弃掉\n",
    "        if sum(ids) == 0:\n",
    "            continue\n",
    "        assert len(ids) == seq_length\n",
    "        # 处理标签，正面为1，负面为0\n",
    "        if i.label == 'positive':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        features.append(data_feature(ids, label))\n",
    "    return features\n",
    "\n",
    "features = convert_example_to_feature(examples)\n",
    "\n",
    "for i in range(3):\n",
    "    print(features[i].ids, features[i].label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后还是熟悉的Dataset和DataLoader，将数据生成Dataset然后用DataLoader去读取~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "ids = torch.tensor([f.ids for f in features], dtype=torch.long)\n",
    "label = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(ids, label)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为网络比较复杂，我们使用Adam优化器，这个优化器和它的改良版AdamW可以说是目前用的最多最广的优化器之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "# 1e-3 ~ 1e-5\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后就是开始训练啦！依然是熟悉的套路："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.490344\n",
      "epoch: 2, loss: 0.301695\n",
      "epoch: 3, loss: 0.222680\n",
      "epoch: 4, loss: 0.173264\n",
      "epoch: 5, loss: 0.149876\n",
      "epoch: 6, loss: 0.118091\n",
      "epoch: 7, loss: 0.099532\n",
      "epoch: 8, loss: 0.087792\n",
      "epoch: 9, loss: 0.080676\n"
     ]
    }
   ],
   "source": [
    "epoch = 9\n",
    "for i in range(epoch):\n",
    "    total_loss = []\n",
    "    for ids, label in dataloader:\n",
    "        # 模型在GPU上的话\n",
    "        if torch.cuda.is_available():\n",
    "            ids = ids.to(torch.device('cuda'))\n",
    "            label = label.to(torch.device('cuda'))\n",
    "        # 因为我们这次loss已经写在模型里面了，所以就不用再计算模型了\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(ids, label)\n",
    "        total_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch: %d, loss: %.6f\" % (i + 1, sum(total_loss) / len(total_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以来检验一下模型的准确性，这时我们之前写forward时用到的“不同参数不同输出”就有用了，在不传入标签的时候，会直接做预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开开心心\n",
      "positive\n",
      "凄凄惨惨\n",
      "negative\n",
      "奥利给\n",
      "positive\n",
      "加油\n",
      "positive\n",
      "杯具\n",
      "positive\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "# 将输出的概率还原成标签\n",
    "# 传入的是一个tensor(gpu?)\n",
    "def tensor_to_label(logits):\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    logits = np.argmax(logits, axis=-1)\n",
    "    if logits[0] == 1:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "# 还记得网络中我们加了dropout吗？\n",
    "# 当我们将model设置为eval状态时，dropout/BatchNorm不生效\n",
    "# model.train()就可以重新回到训练状态\n",
    "model.eval()\n",
    "\n",
    "while True:\n",
    "    s = input()\n",
    "    if s == 'quit':\n",
    "        break\n",
    "    s = [data_example(s, 0)]\n",
    "    s = convert_example_to_feature(s)\n",
    "    ids = torch.tensor([f.ids for f in s], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            ids = ids.to(torch.device('cuda'))\n",
    "        logits = tensor_to_label(model(ids))\n",
    "        print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这节课开始我们正式开始接触各种NLP任务，首先是最简单的全连接做文本分类，大家可以看到5个字的文本分类就已经需要15w+的参数了，因此全连接层在NLP任务的前几层是很少见的，一般会用在最后面几层。之后我们会再给大家讲讲CNN方法做文本分类。  \n",
    "此外，这节课的代码其实也是一般NLP任务的常用代码框架，希望大家能够读懂代码结构，以后能够很大程度少走弯路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}