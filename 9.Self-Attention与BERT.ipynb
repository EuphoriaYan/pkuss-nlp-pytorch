{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention与Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT模型是Google在2019年提出的模型，一经提出就刷了几乎所有榜单的SOTA，可以说是NLP发展上里程碑级别的模型。同时也让原本一步到位的训练变成了现在pre-train，fine-tune两部分的训练。pre-train部分消耗的计算资源非常庞大，往往由大公司/高校来做，而我们要做的往往只是在已经预训练好的模型上，进行下游任务的fine-tune即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{red}{警告：}$运行Bert-base的训练大约需要8GB的内存/显存，不能保证大家的电脑都能跑的起来，如果电脑配置不够且不要直接运行网上下载下来的Bert训练代码，容易发生OOM。  \n",
    "本notebook的代码因为数据规模很小，经测试大约占用2GB的内存/显存，大部分同学的电脑应该都还跑得动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from models.BertBasicModel import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以把Bert理解成一个性能非常好的，会根据上下文内容自动调整的Embedding。我们这里以Bert-base为例，它会为每个字输出一个768维的向量，这个向量不单单是这个词自己的向量，还融合了上下文信息。关于预训练的Mask方法这里就不涉及了，网上有海量的Bert分析文章。也可以查看`models.BertBasciModel`里的`BertForPreTraining`类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于模型很大这里没法给大家直接传到Github上，需要大家自己下载。如果大家想自己试试这个notebook，可以下载来试试。  \n",
    "下载的Bert模型（pytorch版）一般会有三个文件：  \n",
    "1. bert_config.json —— 存储这个Bert模型的各种超参数，例如dropout，hidden_size，多头注意力的头数等；\n",
    "2. pytorch_model.bin —— 存储Bert模型的各种训练参数；\n",
    "3. vocab.txt —— 词表；\n",
    "\n",
    "其中最大的自然是pytorch_model.bin了。大小大概是400MB左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert的路径，各位自行修改，这个是google的标准中文Bert-base\n",
    "bert_path = 'chinese_L-12_H-768_A-12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert需要Bert对应的Tokenizer。这个Tokenizer需要Bert模型所在的目录路径或者vocab.txt的路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tokenizer import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了Tokenizer，我们就可以处理数据了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.apply_text_norm import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Bert的输入需要以下这些Tensor\n",
    "# input_ids是转化为ids的文本输入\n",
    "# input_mask是哪些地方是有文本的\n",
    "# segment_ids是哪些地方是第一句，哪些地方是第二句\n",
    "# label_id是转化为ids的标签输入\n",
    "class InputFeature(object):\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "# 我们使用一个函数让我们之前的那个data_example转化为Bert版本的InputFeature\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in tqdm(enumerate(examples)):\n",
    "        tokens_a = tokenizer.tokenize(process_sent(example.text))\n",
    "        tokens_b = None\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "            \n",
    "        # Bert的特殊token，[CLS]和[SEP]\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        # segment_ids是用来指示第一句还是第二句的\n",
    "        # 0是第一句，1是第二句\n",
    "        # 我们这种只有第一句的任务的话其实就是全0的Tensor\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        # 转换成ids\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        # 目前这些长度都是要输入的，所以input_mask全是1\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # 用0填充剩下的位置\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if len(example.label) > max_seq_length - 2:\n",
    "            example.label = example.label[: (max_seq_length - 2)]\n",
    "\n",
    "        label_id = [label_map[\"B\"]] + [label_map[tmp] for tmp in example.label]\n",
    "        label_id += (len(input_ids) - len(label_id)) * [label_map[\"B\"]]\n",
    "\n",
    "        features.append(InputFeature(input_ids=input_ids,\n",
    "                                     input_mask=input_mask, \n",
    "                                     segment_ids=segment_ids,\n",
    "                                     label_id=label_id))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_examples:  6083\n",
      "Dev_examples:  1107\n",
      "train examples\n",
      "春秋左传定公\n",
      "['B', 'I', 'B', 'I', 'B', 'I']\n",
      "元年\n",
      "['B', 'I']\n",
      "春\n",
      "['B']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6083it [00:00, 23638.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev examples\n",
      "春秋左传隐公\n",
      "['B', 'I', 'B', 'I', 'B', 'I']\n",
      "惠公元妃孟子\n",
      "['B', 'I', 'B', 'I', 'B', 'I']\n",
      "孟子卒\n",
      "['B', 'I', 'B']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1107it [00:00, 23110.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# 我们还是来做一下左传的CWS吧，这样不用再写一遍dataset_loader了（其实就是我懒）\n",
    "from dataset_readers.cws import *\n",
    "# 实例化一个readers\n",
    "data_processor = Zuozhuan_Cws()\n",
    "# 指定max_seq_length为128\n",
    "max_seq_length = 128\n",
    "# 指定batch_size为32\n",
    "batch_size = 32\n",
    "# 标签一共有两种（B和I）\n",
    "num_labels = 2\n",
    "\n",
    "# 获取labels\n",
    "label_list = data_processor.get_labels()\n",
    "\n",
    "# 获取训练语料\n",
    "train_examples = data_processor.get_train_examples()\n",
    "# 获取测试语料\n",
    "dev_examples = data_processor.get_dev_examples()\n",
    "\n",
    "print(\"Train_examples: \", len(train_examples))\n",
    "print(\"Dev_examples: \", len(dev_examples))\n",
    "\n",
    "def generate_data(examples, set_type=\"train\"):\n",
    "    print(set_type + \" examples\")\n",
    "    for i in range(min(len(examples), 3)):\n",
    "        print(examples[i].text)\n",
    "        print(examples[i].label)\n",
    "    sys.stdout.flush()\n",
    "    features = convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n",
    "    input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    data = TensorDataset(input_ids, segment_ids, input_mask, label_ids)\n",
    "    sampler = RandomSampler(data)\n",
    "    return data, sampler\n",
    "\n",
    "# convert data example into features\n",
    "train_data, train_sampler = generate_data(train_examples, \"train\")\n",
    "dev_data, dev_sampler = generate_data(dev_examples, \"dev\")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train steps:  955\n"
     ]
    }
   ],
   "source": [
    "# 指定训练5个epoch\n",
    "# 因为Bert的优化器比较复杂，使用了Warmup策略，所以需要事先知道要跑多少步\n",
    "num_train_epochs = 5\n",
    "# 估算出一共要多少步\n",
    "num_train_steps = int(math.ceil(len(train_examples) / batch_size) * num_train_epochs)\n",
    "print(\"Total train steps: \", num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要做的，就是读进来这个Bert模型，然后在它的基础上加几个我们需要的层。比如序列标注，一般是在最后加FC层和CRF层；分类问题，一般只需要FC层。  \n",
    "这里以一个序列标注为例（为了你们的大作业着想），但是只加了一个FC层，没有加CRF层，保留一些实现的空间给大家~  \n",
    "需要的bert_model_path就是Bert的目录位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please notice that the bert grad is false.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTagger(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BertTagger(nn.Module):\n",
    "    # 接收几个参数：\n",
    "    # bert_model_path: bert的预训练文件的位置\n",
    "    # bert_frozen: 是否冻结bert，让bert不进行训练\n",
    "    # num_labels: 输出的标签个数\n",
    "    def __init__(self, bert_model_path, bert_frozen=True, num_labels=4):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        # 我们必须要一个config来初始化Bert，也就是之前下载下来的文件里的bert_config.json\n",
    "        CONFIG_NAME = 'bert_config.json'\n",
    "        config_path = os.path.join(bert_model_path, CONFIG_NAME)\n",
    "        # 将这个json文件转化为一个BertConfig实例\n",
    "        bert_config = BertConfig.from_json_file(config_path)\n",
    "        # BertModel继承于PreTrainedBertModel类，这个类里实现了from_pretrained功能\n",
    "        self.bert = BertModel.from_pretrained(bert_model_path)\n",
    "        \n",
    "        # 如果bert_frozen为true，就让所有bert里的参数都不需要记录梯度\n",
    "        # 为了效率我们这里就冻结掉Bert的参数吧\n",
    "        if bert_frozen:\n",
    "            print(\"Please notice that the bert grad is false.\")\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # 记录下hidden_size，也就是最终的输出的维度\n",
    "        self.hidden_size = bert_config.hidden_size\n",
    "        # dropout率也沿用之前bert里的dropout率\n",
    "        self.dropout = nn.Dropout(bert_config.hidden_dropout_prob)\n",
    "        # 输出层，简单的一个线性层，从hidden_size映射到num_labels\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_labels) \n",
    "\n",
    "    # 我们输入的一般是(input_ids, segment_ids, input_mask, label_ids)\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        last_bert_layer, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
    "                                                   output_all_encoded_layers=False)\n",
    "        last_bert_layer = last_bert_layer.view(-1, self.hidden_size)\n",
    "        last_bert_layer = self.dropout(last_bert_layer)\n",
    "        logits = self.classifier(last_bert_layer)\n",
    "        # (batch_size, seq_length, num_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            # 带mask的交叉熵\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # 我们知道，实际上很多句子根本没有到seq_length那么长，那么多余的位置我们应该让模型预测什么呢？\n",
    "            # 所以这里使用了attention_mask，其实也就是input_mask，来确定我们关心哪些位置\n",
    "            # 我们只需要考虑attention_mask = 1 位置上的loss，其他地方模型预测什么，我们不关心\n",
    "            active_loss = (attention_mask.view(-1) == 1)\n",
    "            active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
    "            active_label = labels.view(-1)[active_loss]\n",
    "            loss = loss_fct(active_logits, active_label)\n",
    "            return loss\n",
    "        else:\n",
    "            # 直接返回logits\n",
    "            return logits\n",
    "\n",
    "\n",
    "model = BertTagger(bert_path, num_labels=num_labels)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取训练的数据集，同时得到了word_cnt之后就能初始化网络了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来准备Bert专用的Adam优化器，这个优化器相比普通的Adam，加入了梯度修剪和warmup功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertAdam (\n",
      "Parameter Group 0\n",
      "    b1: 0.9\n",
      "    b2: 0.999\n",
      "    e: 1e-06\n",
      "    lr: 5e-05\n",
      "    max_grad_norm: 1.0\n",
      "    schedule: warmup_linear\n",
      "    t_total: 955\n",
      "    warmup: 0.1\n",
      "    weight_decay: 0.01\n",
      "\n",
      "Parameter Group 1\n",
      "    b1: 0.9\n",
      "    b2: 0.999\n",
      "    e: 1e-06\n",
      "    lr: 5e-05\n",
      "    max_grad_norm: 1.0\n",
      "    schedule: warmup_linear\n",
      "    t_total: 955\n",
      "    warmup: 0.1\n",
      "    weight_decay: 0.0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from utils.optimization import BertAdam\n",
    "# 梯度裁剪，超过这个数字会被强制裁剪到这个数字\n",
    "clip_grad = 1.0\n",
    "# warmup率，前多少比例的步，lr逐渐升高\n",
    "warmup_proportion = 0.1\n",
    "# 学习率\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "# bias和LayerNorm的参数不需要权重衰减，不然可能LayerNorm效果会不对\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
    "    {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=learning_rate,\n",
    "                     warmup=warmup_proportion,\n",
    "                     t_total=num_train_steps,\n",
    "                     max_grad_norm=clip_grad)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练。一个epoch昨天看了看大概是40s，如果开着直播可能还会更慢点，可能1min一个epoch？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/5\n",
      "loss: 0.356978\n",
      "f1 score: 0.6367\n",
      "epoch time: 36 s\n",
      "epoch 2/5\n",
      "loss: 0.218270\n",
      "f1 score: 0.7343\n",
      "epoch time: 37 s\n",
      "epoch 3/5\n",
      "loss: 0.204123\n",
      "f1 score: 0.7534\n",
      "epoch time: 37 s\n",
      "epoch 4/5\n",
      "loss: 0.198365\n",
      "f1 score: 0.7607\n",
      "epoch time: 37 s\n",
      "epoch 5/5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(model, optimizer, train_dataloader, epoch=5):\n",
    "    total_start_time = time.time()\n",
    "    for i in range(epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        print(\"epoch %d/%d\" % (i + 1, epoch))\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "        for batch in train_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, segment_ids, input_mask, label_ids = batch\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            total_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"loss: %.6f\" % (sum(total_loss) / len(total_loss)))\n",
    "        epoch_end_time = time.time()\n",
    "        \n",
    "        model.eval()\n",
    "        total_gold = []\n",
    "        total_pred = []\n",
    "        for batch in dev_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, segment_ids, input_mask, label_ids = batch\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, segment_ids, input_mask)\n",
    "                # 直接在最后一维上找哪个最大\n",
    "                logits = torch.argmax(logits, dim=-1)\n",
    "                active_loss = (input_mask.view(-1) == 1)\n",
    "                active_logits = logits.view(-1)[active_loss]\n",
    "                active_label = label_ids.view(-1)[active_loss]\n",
    "                total_gold += active_label.detach().cpu().numpy().tolist()\n",
    "                total_pred += active_logits.detach().cpu().numpy().tolist()\n",
    "        print(\"f1 score: %.4f\" % (f1_score(total_gold, total_pred)))\n",
    "        \n",
    "        print(\"epoch time: %d s\" % (epoch_end_time - epoch_start_time))\n",
    "    total_end_time = time.time()\n",
    "    print(\"total time: %d s\" % (total_end_time - total_start_time))\n",
    "\n",
    "train(model, optimizer, train_dataloader, num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
