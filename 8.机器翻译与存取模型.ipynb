{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq进行机器翻译与存取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当前最复杂的NLP应用之一就是机器翻译，我们今天来讲一个使用Seq2seq模型进行机器翻译的例子，顺便给大家介绍一下存取模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们本次就不用emb啦，因为我们今天的任务是英语翻译成法语，而英语的emb处理起来要花的时间太多了，就直接初始化吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们来建立一个Seq2seq的网络。一般的Seq2seq网络都主要分成两个部分：Encoder和Decoder。其中Encoder是解码器，负责将一个句子转化成一个张量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder部分，其实就是一个标准的RNN网络\n",
    "class EncoderRNN(nn.Module):\n",
    "    # 这里的input_size其实就是英语的词表大小，hidden_size是超参\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # 词嵌入层，这里没有初始化，就让它随着训练自己计算吧\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # 注意这里用了batch_first，所以接收的输入是(batch_size, seq_length, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder是解码器，用于接收Encoder的张量表示，然后通过一定的方法依次输出需要的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder部分，我们这里先用标准的RNN网络，实际上现在大部分是使用带Attention的RNN网络\n",
    "class DecoderRNN(nn.Module):\n",
    "    # 这里的output_size其实就是法语的词表大小，hidden_size必须要和刚才Encoder的hidden_size一致\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # 注意这里用了batch_first，所以接收的输入是(batch_size, seq_length, hidden_size)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # 输入的x是(batch_size)大小，经过embedding后变成(batch_size, hidden_size)，但还是和GRU的要求不一致\n",
    "        # 好在decoder中，seq_length始终是1，所以我们只需要用unsqueeze函数，在中间加一维即可\n",
    "        # (batch_size, hidden_size) -> (batch_size, 1, hidden_size) -> (batch_size, seq_length=1, hidden_size)\n",
    "        embedded = self.dropout(self.embedding(x).unsqueeze(1))\n",
    "        # 通过RNN，得到下一个输出，以及输出对应的hidden_state\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        # 用fc输出层进行预测\n",
    "        prediction = self.out(output)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我们建立一个完整的Seq2seq网络，包含Encoder，Decoder和一些辅助的函数/层。\n",
    "![avatar](素材/seq2seq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq_translater(nn.Module):\n",
    "    # input_size: 英文词表数\n",
    "    # hidden_size: 超参\n",
    "    # output_size: 法文词表数\n",
    "    # seq_length: 最长序列长度\n",
    "    def __init__(self, input_size, hidden_size, output_size, seq_length):\n",
    "        super().__init__()\n",
    "        # 用上之前的Encoder\n",
    "        self.encoder = EncoderRNN(input_size, hidden_size)\n",
    "        # 用上之前的Decoder\n",
    "        self.decoder = DecoderRNN(hidden_size, output_size)\n",
    "        # 小小的注意事项：hidden_size，num_layers必须一致，不然报错\n",
    "        # 用交叉熵损失函数\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        # 这两个属性存下来，还要用到\n",
    "        self.output_size = output_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # 别忘记了我们在GRU里使用了batch_first，所以输入和输出的shape都是batch在最前面的\n",
    "        # output是(batch_size, seq_length, hidden_size)\n",
    "        # LSTM -> output, (hidden_stat, cell_state)\n",
    "        # 但是hidden(以及如果你用LSTM，还多一个cell)，还是原来的样子\n",
    "        # hidden是(num_layers*num_directions, batch_size, hidden_size)\n",
    "        output, hidden = self.encoder(x)\n",
    "        # 用BOS(BEGIN OF SENTENCE)来做Decoder的初始输入，我们可以直接从输入中提取\n",
    "        # 每一轮的输入的shape就是个(batch_size)，一维的\n",
    "        decoder_input = x[:, 0]\n",
    "        # 依次存下每轮的输出\n",
    "        outputs = []\n",
    "        # 一轮一轮地进行迭代\n",
    "        for i in range(self.seq_length):\n",
    "            # decoder迭代一次\n",
    "            output, hidden = self.decoder(decoder_input, hidden)\n",
    "            # 存在outputs里面\n",
    "            outputs.append(output)\n",
    "            # 这个时候的output是(batch_size, 1, output_size)，我们在最后一维上做argmax，就能得到输出的结果\n",
    "            # 但是别忘记了，输入是(batch_size)，所以我们需要进行一个squeeze，把当中那个1去了\n",
    "            pred = output.squeeze(dim=1).argmax(dim=-1)\n",
    "            # 下一轮的输入就是本轮的预测\n",
    "            decoder_input = pred\n",
    "        # 最后我们把所有预测的连起来，在当中那维连起来\n",
    "        # (batch_size, 1, output_size) -> (batch_size, seq_length, output_size)\n",
    "        total_output = torch.cat(outputs, dim=1)\n",
    "        # 还是一样，如果有y就输出loss，没有y就输出预测\n",
    "        if y is not None:\n",
    "            return self.loss_fct(total_output.view(-1, self.output_size), y.view(-1))\n",
    "        else:\n",
    "            return total_output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是有没有发现，我们需要初始化模型的话，还需要俩参数——英语词表大小和法语词表大小，这个我们没法直接确定，需要先读语料库才行。Tokenizer和data_utils已经基于这个notebook进行更新啦，加入了一些新功能~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对英语翻译法语的语料库的dataset_readers\n",
    "from dataset_readers.trans import *\n",
    "# Tokenizer进行了更新，加入了normlizeString和get_vocabs两个类方法，不需要实例即可使用\n",
    "from utils.tokenizer import Tokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def load_data(seq_length, batch_size):\n",
    "    # 实例化一个readers\n",
    "    data_loader = En2Fr_Trans()\n",
    "    # 获取训练语料\n",
    "    train_examples = data_loader.get_train_examples()\n",
    "    # 英语的数据\n",
    "    en_lines = [Tokenizer.normalizeString(i.text_a) for i in train_examples]\n",
    "    # 法语的数据\n",
    "    fr_lines = [Tokenizer.normalizeString(i.text_b) for i in train_examples]\n",
    "    \n",
    "    # 我们需要两个tokenizer，一个用于英语一个用于法语\n",
    "    tokenizer_a = Tokenizer(Tokenizer.get_vocabs(en_lines))\n",
    "    tokenizer_b = Tokenizer(Tokenizer.get_vocabs(fr_lines))\n",
    "    # 得到英语和法语词表大小，现在就可以初始化model啦\n",
    "    word_cnt_a = len(tokenizer_a.vocab)\n",
    "    word_cnt_b = len(tokenizer_b.vocab)\n",
    "    \n",
    "    # 但还是让我们先把数据读完吧，这个函数用于将examples转换成features，然后再生成DataLoader\n",
    "    def generate_dataloader(examples, tokenizer_a, tokenizer_b, seq_length):\n",
    "        features = convert_sents_pair(examples, tokenizer_a, tokenizer_b, seq_length)\n",
    "        text_a = torch.tensor([f.text_a for f in features], dtype=torch.long)\n",
    "        text_b = torch.tensor([f.text_b for f in features], dtype=torch.long)\n",
    "        dataset = TensorDataset(text_a, text_b)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return dataloader\n",
    "    # 生成DataLoader\n",
    "    train_dataloader = generate_dataloader(train_examples, tokenizer_a, tokenizer_b, seq_length)\n",
    "    return train_dataloader, word_cnt_a, word_cnt_b, tokenizer_a, tokenizer_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取训练的数据集，同时得到了word_cnt之后就能初始化网络了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2seq_translater(\n",
      "  (encoder): EncoderRNN(\n",
      "    (embedding): Embedding(13043, 256)\n",
      "    (gru): GRU(256, 256, batch_first=True)\n",
      "  )\n",
      "  (decoder): DecoderRNN(\n",
      "    (embedding): Embedding(21334, 256)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (gru): GRU(256, 256, batch_first=True)\n",
      "    (out): Linear(in_features=256, out_features=21334, bias=True)\n",
      "  )\n",
      "  (loss_fct): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 我们做机器翻译啦，这个时候序列长度就需要比较长了，作为实验我们先使用64\n",
    "seq_length = 64\n",
    "# 我在预训练的过程中，batch_size是64，大家可以视自己情况进行调整\n",
    "batch_size = 32\n",
    "train_dataloader, word_cnt_a, word_cnt_b, tokenizer_en, tokenizer_fr = load_data(seq_length, batch_size)\n",
    "# 超参hidden_state设为256，只是随手设置的，你也可以自己调整\n",
    "model = Seq2seq_translater(word_cnt_a, 256, word_cnt_b, seq_length)\n",
    "# 我之前在常老师的服务器上跑了一遍，我们来读取模型\n",
    "# model.save() -> model.load()\n",
    "# torch.save(model.state_dict()) -> model.load_state_dict(torch.load())\n",
    "# ELECTRA -> BERT 上面的就不行了，下面的可以\n",
    "model.load_state_dict(torch.load('./models/seq2seq_translate.bin'))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(torch.device('cuda'))\n",
    "# 使用print可以打印出网络的结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依然使用Adam优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练。这里就不运行训练了，因为这个一个epoch估计是10分钟左右了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, optimizer, train_dataloader, epoch=5):\n",
    "    total_start_time = time.time()\n",
    "    for i in range(epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        print(\"epoch %d/%d\" % (i + 1, epoch))\n",
    "        model.train()\n",
    "        total_loss = []\n",
    "        for text_a, text_b in train_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                text_a = text_a.to(torch.device('cuda'))\n",
    "                text_b = text_b.to(torch.device('cuda'))\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(text_a, text_b)\n",
    "            total_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"epoch: %d, loss: %.6f\" % (i + 1, sum(total_loss) / len(total_loss)))\n",
    "        epoch_end_time = time.time()\n",
    "        print(\"epoch time: %d s\" % (epoch_end_time - epoch_start_time))\n",
    "        torch.save(model.state_dict(), './models/seq2seq_translate.bin')\n",
    "    total_end_time = time.time()\n",
    "    print(\"total time: %d s\" % (total_end_time - total_start_time))\n",
    "\n",
    "epoch = 1\n",
    "train(model, optimizer, train_dataloader, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后做一下测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good afternoon.\n",
      "lavee nos arriverai recalee ferree\n",
      "convert\n",
      "efficacement maturite segment\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer_fr.vocab\n",
    "id2word = {v: k for k,v in vocab.items()}\n",
    "\n",
    "def tensor2text(pred):\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    pred = np.argmax(pred, axis=-1)\n",
    "    output = []\n",
    "    for i in pred:\n",
    "        w = id2word[i]\n",
    "        if w == 'EOS':\n",
    "            break\n",
    "        output.append(w)\n",
    "    return ' '.join(output)\n",
    "\n",
    "while True:\n",
    "    s = input()\n",
    "    if s == 'quit':\n",
    "        break\n",
    "    s = [sents_pair_example(s, '')]\n",
    "    s = convert_sents_pair(s, tokenizer_en, tokenizer_fr, seq_length)\n",
    "    text_a = torch.tensor([f.text_a for f in s], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            text_a = text_a.to(torch.device('cuda'))\n",
    "        text_b = model(text_a)\n",
    "        print(tensor2text(text_b))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "看起来不太行……主要是网络结构，数据，训练时间啥的都不太够，就当做给大家的一个参考吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
